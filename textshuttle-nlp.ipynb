{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cce1gxUyS4Ui"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "\n",
        "    def __init__(self, path: Path):\n",
        "        \"\"\"\n",
        "        Load segments from a file, one per line.\n",
        "        \"\"\"\n",
        "        with path.open(encoding='utf-8') as corpus:\n",
        "            self.segments = [s.strip('\\n') for s in corpus]\n",
        "\n",
        "    def filter(self, min_alpha: float = 0.5):\n",
        "        \"\"\"\n",
        "        Discard all segments with too few alphabetical characters.\n",
        "\n",
        "        Keep only segments where at least `min_alpha` of the\n",
        "        characters are alphabetic.\n",
        "\n",
        "        Examples for min_alpha == 0.5:\n",
        "            \"So.\"      # keep\n",
        "            \"¡Sí!\"     # keep\n",
        "            \"Ja!!!\"    # discard\n",
        "            \"123.5\"    # discard\n",
        "        \"\"\"\n",
        "        pass  # TODO: Task 1\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Replace fancy quotes like « » „ “ ” with ASCII quotes (\").\n",
        "        \"\"\"\n",
        "        pass  # TODO: Task 1\n",
        "\n",
        "    def split(self, language: str = 'en'):\n",
        "        \"\"\"\n",
        "        Split up segments containing multiple sentences using an online API.\n",
        "        Remove any leading and trailing whitespace.\n",
        "\n",
        "        Example:\n",
        "            Before: self.segments = [\"Wie geht's?\", \"Ausgezeichnet. Und dir?\"]\n",
        "            After:  self.segments = [\"Wie geht's?\", \"Ausgezeichnet.\", \"Und dir?\"]\n",
        "        \"\"\"\n",
        "        pass  # TODO: Task 2\n",
        "\n",
        "\n",
        "# TODO: Task 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "\n",
        "    def __init__(self, path: Path):\n",
        "        \"\"\"\n",
        "        Load segments from a file, one per line.\n",
        "        \"\"\"\n",
        "        with path.open(encoding='utf-8') as corpus:\n",
        "            self.segments = [s.strip('\\n') for s in corpus]\n",
        "\n",
        "    def filter(self, min_alpha: float = 0.5):\n",
        "        \"\"\"\n",
        "        Discard all segments with too few alphabetical characters.\n",
        "\n",
        "        Keep only segments where at least `min_alpha` of the\n",
        "        characters are alphabetic.\n",
        "\n",
        "        Examples for min_alpha == 0.5:\n",
        "            \"So.\"      # keep\n",
        "            \"¡Sí!\"     # keep\n",
        "            \"Ja!!!\"    # discard\n",
        "            \"123.5\"    # discard\n",
        "        \"\"\"\n",
        "        new_segments = [] # segments after filtering\n",
        "        for s in self.segments:\n",
        "            # iterate over all segments\n",
        "            alpha_count = 0 # count of alphabetic characters\n",
        "            for i in s:\n",
        "                if i.isalpha(): # whether character i is alpha\n",
        "                    alpha_count += 1\n",
        "            if alpha_count / len(s) >= min_alpha:\n",
        "                new_segments.append(s)\n",
        "\n",
        "        return new_segments  # TODO: Task 1\n",
        "\n",
        "    def normalize(self):\n",
        "        \"\"\"\n",
        "        Replace fancy quotes like « » „ “ ” with ASCII quotes (\").\n",
        "        \"\"\"\n",
        "\n",
        "        new_segments = [] # segments after filtering\n",
        "        for s in self.segments:\n",
        "            # iterate over all segments\n",
        "            alpha_count = 0 # count of alphabetic characters\n",
        "            new_s = \"\" # new segment, initialized to an empty string\n",
        "            for i in s: #we need a range to input i into s\n",
        "                if i in [\"«\", \"»\", \"„\", \"“\", \"”\"]: # whether character i is a fancy quote\n",
        "                    new_s = new_s + '\"'\n",
        "                else:\n",
        "                    new_s = new_s + i\n",
        "            new_segments.append(new_s)\n",
        "\n",
        "        return new_segments  # TODO: Task 1\n",
        "\n",
        "    def split(self, language: str = 'en'):\n",
        "        \"\"\"\n",
        "        Split up segments containing multiple sentences using an online API.\n",
        "        Remove any leading and trailing whitespace.\n",
        "\n",
        "        Example:\n",
        "            Before: self.segments = [\"Wie geht's?\", \"Ausgezeichnet. Und dir?\"]\n",
        "            After:  self.segments = [\"Wie geht's?\", \"Ausgezeichnet.\", \"Und dir?\"]\n",
        "        \"\"\"\n",
        "        pass  # TODO: Task 2\n",
        "\n",
        "\n",
        "# TODO: Task 3"
      ],
      "metadata": {
        "id": "u4-mgiqVTVPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "TWtz6G5iXMY4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.post(\n",
        "    url='https://stg.tait.ts.mt/api/v2/split_sentences', \n",
        "    headers={'accept': 'application/json', 'Content-type':'application/json'},\n",
        "    data=r'{\"text\": \"Winston went up the stairs.   In the hallway, it smelled like boiled cabbage and damp floor mats.\\n It was disgusting!\", \"language\": \"en\"}', \n",
        "    )"
      ],
      "metadata": {
        "id": "XeLg2WQbXQOp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "HpiFaXn1dUI5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval((r.content))"
      ],
      "metadata": {
        "id": "4ylH6lMSYckg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c4b879-1621-4e2d-d6d0-a99e9ce64c8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Winston went up the stairs.   ',\n",
              " 'In the hallway, it smelled like boiled cabbage and damp floor mats.\\n',\n",
              " ' It was disgusting!']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_eTswNZVVrsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}